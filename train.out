started latent_dim 256
Epoch 0  Loss:3081695.0
Epoch 10  Loss:2747432.75
Epoch 20  Loss:1518198.5
Epoch 30  Loss:1172883.75
Epoch 40  Loss:1015191.625
Epoch 50  Loss:914409.0625
Epoch 60  Loss:843809.1875
Epoch 70  Loss:787767.875
Epoch 80  Loss:744695.125
Epoch 90  Loss:715261.75
Epoch 100  Loss:693195.875
Epoch 110  Loss:677328.375
Epoch 120  Loss:664449.25
Epoch 130  Loss:653839.25
Epoch 140  Loss:645563.1875
/orfeo/cephfs/home/dssc/acesa000/Street_View_Generator/environment/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/orfeo/cephfs/home/dssc/acesa000/Street_View_Generator/environment/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Epoch 150  Loss:638665.6875
Epoch 160  Loss:631732.625
Epoch 170  Loss:626894.75
Epoch 180  Loss:623076.875
Epoch 190  Loss:619102.25
Epoch 200  Loss:616097.75
Epoch 210  Loss:614472.4375
Epoch 220  Loss:611531.25
Epoch 230  Loss:608626.0625
Epoch 240  Loss:607014.25
Epoch 250  Loss:606868.625
Epoch 260  Loss:604556.5
Epoch 270  Loss:602052.75
Epoch 280  Loss:601639.25
Epoch 290  Loss:599448.5
Epoch 300  Loss:597725.3125
Epoch 310  Loss:596711.75
Epoch 320  Loss:597801.6875
Epoch 330  Loss:595665.25
Epoch 340  Loss:595094.4375
Epoch 350  Loss:593975.25
Epoch 360  Loss:592766.1875
Epoch 370  Loss:592071.375
Epoch 380  Loss:591363.375
Epoch 390  Loss:590808.5625
Epoch 400  Loss:590739.6875
Epoch 410  Loss:590833.375
Epoch 420  Loss:589680.5625
Epoch 430  Loss:589339.0
Epoch 440  Loss:589262.0
Epoch 449  Loss:589011.9375
Completed training with lowest loss: 588698.25 reached at EPOCH: 447; Time: 58.03260095914205
done latent_dim 256
done
