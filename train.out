started latent_dim 256
Epoch 0  Loss:1798751.375
Epoch 10  Loss:714437.25
Epoch 20  Loss:653335.8125
Epoch 30  Loss:623680.25
/orfeo/cephfs/home/dssc/acesa000/Street_View_Generator/environment/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/orfeo/cephfs/home/dssc/acesa000/Street_View_Generator/environment/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Epoch 40  Loss:613347.875
Epoch 50  Loss:602847.375
Epoch 60  Loss:598893.75
Epoch 70  Loss:593125.75
Epoch 80  Loss:585159.75
Epoch 90  Loss:586488.0
Epoch 100  Loss:580063.75
Epoch 110  Loss:576532.125
Epoch 120  Loss:569862.25
Epoch 130  Loss:570867.75
Epoch 140  Loss:567141.1875
Epoch 150  Loss:559765.875
Epoch 160  Loss:557494.875
Epoch 170  Loss:556660.5625
Epoch 180  Loss:551866.25
Epoch 190  Loss:549141.875
Epoch 200  Loss:550511.125
Epoch 210  Loss:545205.0
Epoch 220  Loss:544014.875
Epoch 230  Loss:544285.6875
Epoch 240  Loss:540575.5
Epoch 250  Loss:539495.75
Epoch 260  Loss:540244.0625
Epoch 270  Loss:536113.8125
Epoch 280  Loss:533377.375
Epoch 290  Loss:530483.5
Epoch 300  Loss:530331.0
Epoch 310  Loss:527803.75
Epoch 320  Loss:526306.5
Epoch 330  Loss:525197.625
Epoch 340  Loss:524351.375
Epoch 349  Loss:523355.1875
Completed training with loss: 523355.1875; Time: 52.13108201424281
done latent_dim 256
done
